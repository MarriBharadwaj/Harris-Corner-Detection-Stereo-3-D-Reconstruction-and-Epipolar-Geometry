# -*- coding: utf-8 -*-
"""B21CS045_qu2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QaU8xXWCM5y8o5qd9XL5nBA_2fUkZEND
"""

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

focal_length_1_x = 5299.313
focal_length_1_y = 5299.313
principal_point_1_x = 1263.818
principal_point_1_y = 977.763

focal_length_2_x = 5299.313
focal_length_2_y = 5299.313
principal_point_2_x = 1438.004
principal_point_2_y = 977.763

baseline=177.288

K1 = np.array([[focal_length_1_x, 0, principal_point_1_x],
               [0, focal_length_1_y, principal_point_1_y],
               [0, 0, 1]])
K2 = np.array([[focal_length_2_x, 0, principal_point_2_x],
               [0, focal_length_2_y, principal_point_2_y],
               [0, 0, 1]])

img_left = cv2.imread('/content/bikeL.png', 0)
img_right = cv2.imread('/content/bikeR.png', 0)

img1 = img_left.copy()
img2 = img_right.copy()
gray1 = img1
gray2 = img2
sift = cv2.SIFT_create()
kp1,des1 = sift.detectAndCompute(gray1, None)
kp2,des2 = sift.detectAndCompute(gray2, None)
img_1=cv2.drawKeypoints(gray1,kp1,img1)
img_2=cv2.drawKeypoints(gray2,kp2,img2)
bf = cv2.BFMatcher()
matches = bf.match(des1,des2)
matches = sorted(matches, key = lambda x:x.distance)
matches = matches[:150]

pt_img1 =np.array([kp1[match.queryIdx].pt for match in matches]).reshape(-1,2)
pt_img2 = np.array([kp2[match.trainIdx].pt for match in matches]).reshape(-1,2)

"""stereo image I1"""

cv2_imshow(img_left)

"""stereo image I2"""

cv2_imshow(img_right)

"""normalising pixel points using handmade function so that the points are invariant to changes in image's scale and translation"""

def normalise_points(pts):
    mean_ =np.mean(pts,axis=0)

    #finding centre
    u = pts[:,0] - mean_[0]
    v = pts[:,1] - mean_[1]

    sd_u = 1/np.std(pts[:,0])
    sd_v = 1/np.std(pts[:,1])
    Tscale = np.array([[sd_u,0,0],[0,sd_v,0],[0,0,1]])
    Ta = np.array([[1,0,-mean_[0]],[0,1,-mean_[1]],[0,0,1]])
    T = np.dot(Tscale,Ta)

    pt = np.column_stack((pts,np.ones(len(pts))))
    norm_pts = (np.dot(T,pt.T)).T

    return norm_pts,T

"""calculates the fundamental matrix between a pair of sets of corresponding normalised image points/pixels"""

def estimate_F(img1_pts,img2_pts):
    img1_pts,T1 = normalise_points(img1_pts)
    img2_pts,T2 = normalise_points(img2_pts)

    x1 = img1_pts[:,0]
    y1 = img1_pts[:,1]
    x1dash = img2_pts[:,0]
    y1dash = img2_pts[:,1]
    A = np.zeros((len(x1),9))

    for i in range(len(x1)):
        A[i] = np.array([x1dash[i]*x1[i],x1dash[i]*y1[i],x1dash[i], y1dash[i]*x1[i],y1dash[i]*y1[i],y1dash[i],x1[i],y1[i],1])

    #taking SVD of A for estimation of F
    U, S, V = np.linalg.svd(A,full_matrices=True)
    F_est = V[-1, :]
    F_est = F_est.reshape(3,3)

    #forcing rank 2 for F
    ua,sa,va = np.linalg.svd(F_est,full_matrices=True)
    sa = np.diag(sa)
    sa[2,2] = 0
    F = np.dot(ua,np.dot(sa,va))
    F = np.dot(T2.T, np.dot(F, T1))
    return F

"""in this ransac function, I estimated the fundamental matrix 'F' by iteratively selecting random subsets of points and fitting a model to find the best set of inliers"""

def ransac(pt1,pt2):
    n_rows = np.array(pt1).shape[0]
    no_iter = 1000
    threshold = 0.05
    inliers = 0

    final_indices = []
    for i in range(no_iter):
        indices = []

        #randomly selecting eight points
        random = np.random.choice(n_rows,size = 8)
        img1_8pt = pt1[random]
        img2_8pt = pt2[random]

        F_est = estimate_F(img1_8pt,img2_8pt)

        for j in range(n_rows):
            x1 = pt1[j]
            x2 = pt2[j]

            #computing error
            pt1_ = np.array([x1[0],x1[1],1])
            pt2_ = np.array([x2[0],x2[1],1])
            error = np.dot(pt1_.T,np.dot(F_est,pt2_))

            if np.abs(error) < threshold:
                indices.append(j)

        if len(indices) > inliers:
            inliers = len(indices)
            final_indices = indices
            F = F_est
    img1_points = pt1[final_indices]
    img2_points = pt2[final_indices]
    return img1_points,img2_points, F

"""Here, I derive the essential matrix 'E' from the fundamental matrix 'F' and the camera intrinsic matrices "K1" and "K2"
"""

def estimate_E(K1, K2, F):
    return np.dot(np.dot(K2.T, F), K1)

"""fundamental matrix"""

pts1_,pts2_, F = ransac(pt_img1,pt_img2)
print("Estimated F:\n",F)

"""essential matrix"""

E = estimate_E(K1, K2, F)
print("Estimated E:\n",E)

"""Here, I derive the rotation and translation matrices from the essential matrix and corresponding point sets using camera intrinsic matrices "K1" and "K2"
"""

def extract_pose(E, pts1, pts2, K1, K2):
    _, R, T, _ = cv2.recoverPose(E, pts1, pts2, K1)
    return R, T

R,T = extract_pose(E,pts1_,pts2_,K1,K2)

"""rotation matrix"""

R

"""translation matrix"""

T

"""I rectify two images using the given fundamental matrix 'F' and corresponding point sets"""

def rectify(img1,img2,F,pts1,pts2):
    h1,w1 = img1.shape[:2]
    h2,w2 = img2.shape[:2]

    _,H1, H2 = cv2.stereoRectifyUncalibrated(np.float32(pts1), np.float32(pts2), F, imgSize=(w1, h1))
    img1_rectified = cv2.warpPerspective(img1, H1, (w1, h1))
    img2_rectified = cv2.warpPerspective(img2, H2, (w2, h2))

    H1_inv = np.linalg.inv(H1)
    H2_inv = np.linalg.inv(H2)
    F_rectified = np.dot(H2_inv.T, np.dot(F,H1_inv))

    return img1_rectified,img2_rectified,F_rectified

"""I drew lines and points on an image based on a set of lines and corresponding points"""

def draw_lines(lines_set,image,points):
    image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
    for line,pt in zip(lines_set,points):
        x0,y0 = map(int, [0, -line[2]/line[1] ])
        x1,y1 = map(int, [image.shape[1]-1, -line[2]/line[1] ])
        img = cv2.line(image, (x0,y0), (x1,y1), (0,255,0) ,2)
        img = cv2.circle(image, (int(pt[0]),int(pt[1])), 5, (0,255,0), 2)
    return img

"""I computed epipolar lines for two sets of points using the fundamental matrix 'F', and visualised these lines on two images and displayed the computed lines"""

def epipolar_lines(pts1_,pts2_,F,image1,image2):
    lines1_,lines2_ = [], []
    for i in range(len(pts1_)):
        p1 = np.array([pts1_[i,0], pts1_[i,1], 1])
        p2 = np.array([pts2_[i,0], pts2_[i,1], 1])

        lines1_.append(np.dot(F.T, p2))
        lines2_.append(np.dot(F,p1))
    img1 = draw_lines(lines1_,image1,pts1_)
    img2 = draw_lines(lines2_,image2,pts2_)

    out = np.hstack((img1, img2))
    cv2_imshow(out)
    return lines1_,lines2_

img1_rectified, img2_rectified, F_rectified = rectify(img_left, img_right, F, pts1_, pts2_)
lines1, lines2 = epipolar_lines(pts1_, pts2_, F_rectified, img1_rectified, img2_rectified)

"""I extractED blocks of specified window_size from THE image, storing both the blocks and their corresponding locations within the image"""

def get_blocks(img,window_size):
    height,width = img.shape
    window_set=[]
    window_loc = []
    for y in range(window_size, height - window_size):
        for x in range(window_size, width - window_size):
            img_window = img[y:y + window_size, x:x + window_size]
            window_set.append(img_window)
            window_loc.append((y,x))
    return window_set,window_loc

"""boundary conditions"""

def get_boundary_condition(x,y,search_range,width,height):
    range_min = max(0, x-search_range)
    range_max = min(width,x+search_range )
    return range_max,range_min

"""deriving the um of squared differences b/w 2 image windows"""

def get_sdd(imgl_window,imgr_window):
    max = 1e10
    if imgl_window.shape == imgr_window.shape:
        diff = np.abs(imgl_window - imgr_window)
        SSD = np.sum(np.square(imgl_window - imgr_window))
        return SSD
    else:
        return max

"""I iteratively compared each block from the left image with blocks in the right image within the search range, computing the SSD to find the best matching block and calculating the disparity"""

from tqdm import tqdm
def get_disparity(img1,img2,search_range):
        imgl = img1.copy()
        imgr = img2.copy()
        disparityImg = np.zeros(img2.shape)
        window_size = 15
        #get a patch in left image
        height,width = img1.shape

        imgl = imgl.astype(np.int32)
        imgr = imgr.astype(np.int32)

        for y in tqdm(range(0,height-window_size)):

            imgl_window_set = []
            imgr_window_set = []

            for x in range(0,width-window_size):
                left_window = imgl[y:y+window_size, x:x+window_size]
                right_window = imgr[y:y+window_size, x:x+window_size]
                imgl_window_set.append(left_window.flatten())
                imgr_window_set.append(right_window.flatten())

            imgl_window_set = np.array(imgl_window_set)
            imgr_window_set = np.array(imgr_window_set)
            print(len(imgl_window_set))
            for i in range(len(imgl_window_set)):

                SSD = np.sum(np.square(imgr_window_set- imgl_window_set[i]),axis=1)
                index = np.argmin(SSD)
                disparity = np.abs(i - index)
                # disparity = ((disparity+1) / 2) * 255
                disparityImg[y,i] = np.uint8(disparity)
        return disparityImg

"""I calculated the depth using the formula depth and clipped the maximum depth value to 10,000 before normalisation"""

def get_depth(disparityImg,b,f):
    depth = (b * f) / (disparityImg + 1e-10)
    depth[depth > 10000] = 10000
    depth_map = np.uint8(depth * 255 / np.max(depth))
    return depth_map

from matplotlib import pyplot as plt
gray1 = img_left
gray2 = img_right
disparity_map =  get_disparity(gray1,gray2,50)
cv2_imshow(disparity_map)
plt.imshow(disparity_map, cmap="gray")
depth_map = get_depth(disparity_map,baseline,focal_length_1_x)
cv2_imshow(depth_map)
plt.imshow(depth_map, cmap='gray', interpolation='nearest')

"""I computed a 3D point cloud from given disparity map and depth map by calculating the corresponding 3D coordinates (X, Y, Z) based on the disparity and depth values."""

def generate_point_cloud(disparity_map, depth_map, camera_parameters):
    fx, fy = camera_parameters['focal_length']
    cx, cy = camera_parameters['principal_point']

    #deriving 3D coordinates
    points_3d = []
    for v in range(disparity_map.shape[0]):
        for u in range(disparity_map.shape[1]):
            disparity = disparity_map[v, u]
            depth = depth_map[v, u]
            if disparity > 0:
                Z = depth
                X = (u - cx) * Z / fx
                Y = (v - cy) * Z / fy
                points_3d.append([X, Y, Z])

    return np.array(points_3d)

#example:
camera_parameters = {
    'focal_length': (focal_length_1_x, focal_length_1_y),
    'principal_point': (principal_point_1_x, principal_point_1_y)
}
point_cloud = generate_point_cloud(disparity_map, depth_map, camera_parameters)

"""I visualised a point cloud, i.e., created a 3-D scatter plot of the point cloud using blue markers and displayed it with labeled axes for X, Y, and Z dimensions"""

from mpl_toolkits.mplot3d import Axes3D

def display_point_cloud(point_cloud):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2], c='b', marker='.')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    plt.show()

# Display the point cloud
display_point_cloud(point_cloud)